{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Студент Александр Иванов, факультет ИИ, user:2818070"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет одежды. 2 класса. \"Обычная одежда\" и \"Камуфляжная одежда\"\n",
    "https://www.kaggle.com/imneonizer/normal-vs-camouflage-clothes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](https://www.pyimagesearch.com/wp-content/uploads/2020/04/fine_tune_resnet_dataset.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузить архив с данными можно по ссылке https://yadi.sk/d/utrrcRLZttCmNg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from pyimagesearch import config\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Утилита для подсчета графических файлов в директории\n",
    "import os\n",
    "\n",
    "image_types = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")\n",
    "\n",
    "def list_images(basePath, contains=None):\n",
    "    # список графических файлов\n",
    "    return list_files(basePath, validExts=image_types, contains=contains)\n",
    "\n",
    "\n",
    "def list_files(basePath, validExts=None, contains=None):\n",
    "    for (rootDir, dirNames, filenames) in os.walk(basePath): \n",
    "        for filename in filenames:\n",
    "            if contains is not None and filename.find(contains) == -1:\n",
    "                continue\n",
    "\n",
    "            ext = filename[filename.rfind(\".\"):].lower()\n",
    "\n",
    "            if validExts is None or ext.endswith(validExts):\n",
    "                imagePath = os.path.join(rootDir, filename)\n",
    "                yield imagePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основные пути к данным\n",
    "BASE_PATH = \"camo_not_camo\"\n",
    "\n",
    "TRAIN_PATH = os.path.sep.join([BASE_PATH, \"training\"])\n",
    "VAL_PATH = os.path.sep.join([BASE_PATH, \"validation\"])\n",
    "TEST_PATH = os.path.sep.join([BASE_PATH, \"testing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.75\n",
    "VAL_SPLIT = 0.1\n",
    "CLASSES = [\"camouflage_clothes\", \"normal_clothes\"]\n",
    "\n",
    "INIT_LR = 1e-4\n",
    "BS = 32\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "MODEL_PATH = \"camo_detector.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Структура датасета, это 3 папки train, valid, test (внутри 2 папки(2 класса) с изображениями)\n",
    "totalTrain = len(list(list_images(TRAIN_PATH)))\n",
    "totalVal = len(list(list_images(VAL_PATH)))\n",
    "totalTest = len(list(list_images(TEST_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных трэйн через утилиту Керас с разными настройками\n",
    "trainAug = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных для валидации\n",
    "valAug = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбор среднего значения для аргументации\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "trainAug.mean = mean\n",
    "valAug.mean = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10731 images belonging to 2 classes.\n",
      "Found 1192 images belonging to 2 classes.\n",
      "Found 3975 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных через генератор для обучающей, валидационной и тестровой выборок\n",
    "trainGen = trainAug.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    batch_size=BS)\n",
    "\n",
    "valGen = valAug.flow_from_directory(\n",
    "    VAL_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=BS)\n",
    "\n",
    "testGen = valAug.flow_from_directory(\n",
    "    TEST_PATH,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] preparing model...\n",
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 223s 2us/step\n"
     ]
    }
   ],
   "source": [
    "# Загрузка моедли ResNet-50 network\n",
    "print(\"[INFO] preparing model...\")\n",
    "baseModel = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сконструируем \"голову\" нейросети на предобученных данных\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(256, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(len(CLASSES), activation=\"softmax\")(headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поместим \"голову\" в нашу модель\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заморозим веса, чтобы они не обучались\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Компилируем модель\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "WARNING:tensorflow:From <ipython-input-11-d25c4bfcd7bb>:8: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 335 steps, validate for 37 steps\n",
      "Epoch 1/20\n",
      "335/335 [==============================] - 248s 741ms/step - loss: 0.1625 - accuracy: 0.9350 - val_loss: 0.1778 - val_accuracy: 0.9324\n",
      "Epoch 2/20\n",
      "335/335 [==============================] - 240s 715ms/step - loss: 0.0910 - accuracy: 0.9664 - val_loss: 0.1149 - val_accuracy: 0.9578\n",
      "Epoch 3/20\n",
      "335/335 [==============================] - 239s 714ms/step - loss: 0.0790 - accuracy: 0.9711 - val_loss: 0.1322 - val_accuracy: 0.9519\n",
      "Epoch 4/20\n",
      "335/335 [==============================] - 240s 718ms/step - loss: 0.0638 - accuracy: 0.9769 - val_loss: 0.1127 - val_accuracy: 0.9595\n",
      "Epoch 5/20\n",
      "335/335 [==============================] - 239s 714ms/step - loss: 0.0580 - accuracy: 0.9789 - val_loss: 0.1225 - val_accuracy: 0.9578\n",
      "Epoch 6/20\n",
      "335/335 [==============================] - 239s 714ms/step - loss: 0.0560 - accuracy: 0.9801 - val_loss: 0.0978 - val_accuracy: 0.9679\n",
      "Epoch 7/20\n",
      "335/335 [==============================] - 239s 714ms/step - loss: 0.0506 - accuracy: 0.9835 - val_loss: 0.0997 - val_accuracy: 0.9679\n",
      "Epoch 8/20\n",
      "335/335 [==============================] - 239s 713ms/step - loss: 0.0436 - accuracy: 0.9843 - val_loss: 0.1040 - val_accuracy: 0.9662\n",
      "Epoch 9/20\n",
      "335/335 [==============================] - 239s 715ms/step - loss: 0.0411 - accuracy: 0.9856 - val_loss: 0.0984 - val_accuracy: 0.9654\n",
      "Epoch 10/20\n",
      "335/335 [==============================] - 239s 714ms/step - loss: 0.0368 - accuracy: 0.9856 - val_loss: 0.1131 - val_accuracy: 0.9611\n",
      "Epoch 11/20\n",
      "335/335 [==============================] - 239s 715ms/step - loss: 0.0325 - accuracy: 0.9887 - val_loss: 0.0986 - val_accuracy: 0.9662\n",
      "Epoch 12/20\n",
      "335/335 [==============================] - 240s 715ms/step - loss: 0.0337 - accuracy: 0.9877 - val_loss: 0.0890 - val_accuracy: 0.9671\n",
      "Epoch 13/20\n",
      "335/335 [==============================] - 239s 714ms/step - loss: 0.0314 - accuracy: 0.9894 - val_loss: 0.1010 - val_accuracy: 0.9671\n",
      "Epoch 14/20\n",
      "335/335 [==============================] - 239s 715ms/step - loss: 0.0309 - accuracy: 0.9892 - val_loss: 0.0944 - val_accuracy: 0.9679\n",
      "Epoch 15/20\n",
      "335/335 [==============================] - 239s 714ms/step - loss: 0.0300 - accuracy: 0.9893 - val_loss: 0.0972 - val_accuracy: 0.9679\n",
      "Epoch 16/20\n",
      "335/335 [==============================] - 239s 714ms/step - loss: 0.0253 - accuracy: 0.9919 - val_loss: 0.0934 - val_accuracy: 0.9696\n",
      "Epoch 17/20\n",
      "335/335 [==============================] - 239s 715ms/step - loss: 0.0247 - accuracy: 0.9909 - val_loss: 0.1054 - val_accuracy: 0.9654\n",
      "Epoch 18/20\n",
      "335/335 [==============================] - 239s 715ms/step - loss: 0.0262 - accuracy: 0.9910 - val_loss: 0.0947 - val_accuracy: 0.9738\n",
      "Epoch 19/20\n",
      "335/335 [==============================] - 239s 714ms/step - loss: 0.0201 - accuracy: 0.9930 - val_loss: 0.1061 - val_accuracy: 0.9611\n",
      "Epoch 20/20\n",
      "335/335 [==============================] - 240s 715ms/step - loss: 0.0219 - accuracy: 0.9928 - val_loss: 0.0984 - val_accuracy: 0.9696\n"
     ]
    }
   ],
   "source": [
    "# Тренируем модель\n",
    "print(\"[INFO] training model...\")\n",
    "H = model.fit(\n",
    "    trainGen,\n",
    "    steps_per_epoch=totalTrain // BS,\n",
    "    validation_data=valGen,\n",
    "    validation_steps=totalVal // BS,\n",
    "    epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n"
     ]
    }
   ],
   "source": [
    "# Сбросим тестовый генератор и сделаем предсказания\n",
    "print(\"[INFO] evaluating network...\")\n",
    "testGen.reset()\n",
    "predIdxs = model.predict(testGen, steps=(totalTest // BS) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "camouflage_clothes       0.95      0.98      0.97      1968\n",
      "    normal_clothes       0.98      0.95      0.97      2007\n",
      "\n",
      "          accuracy                           0.97      3975\n",
      "         macro avg       0.97      0.97      0.97      3975\n",
      "      weighted avg       0.97      0.97      0.97      3975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# для каждого изображения предскажем класс\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "# Посомтрим метрику\n",
    "print(classification_report(testGen.classes, predIdxs, target_names=testGen.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving model...\n"
     ]
    }
   ],
   "source": [
    "# Сохраним модель на диск\n",
    "print(\"[INFO] saving model...\")\n",
    "model.save(MODEL_PATH, save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'H' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-bc1b87c1def5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ggplot\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_acc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'H' is not defined"
     ]
    }
   ],
   "source": [
    "# Построим график потерь\n",
    "N = NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "# plt.show()\n",
    "plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2](plot.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
